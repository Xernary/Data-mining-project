pt1.data.2 [i, "Class"] <- as.character(row.correct.label)  #pt1_data[i,(pt1_data[1, ]) == 1] #paste("Class", pt1_data[i, ], sep="")
}
#Separiamo i dati in due partizioni: training (75%) e test set (25%) con tuple scelte a caso
perc.splitting <- 0.75
#Calcoliamo il numero di tuple nel training set
nobs.training <- round(perc.splitting*nrow(pt1.data.2))
#Campioniamo in maniera random le tuple
sampled.pos <- sample(1:nrow(pt1.data.2),nobs.training)
#Effettuiamo il partizionamento
pt1.training <- pt1.data.2[sampled.pos,]
pt1.testing <- pt1.data.2[-sampled.pos,]
#Nascondiamo la classe di appartenenza nel test set
true.classes <- pt1.testing[,10]
pt1.testing <- pt1.testing[,-10]
#Carico le libreria necessaria
library(rpart)
#Carichiamo anche rpart.plot per la visualizzazione
library(rpart.plot)
#Vogliamo classificare la tipologia di tumore (campo "Class" in funzione degli altri attributi)
pt1.cart <- rpart(Class ~ ., data = pt1.training)
#Stampo l'albero decisionale ottenuto in forma testuale
pt1.cart
#Per avere maggiori informazioni pi? accurate si pu? usare il metodo "printcp"
#che mostra anche il cost-complexity pruning (CP)
#nsplit ? il numero di split necessari per arrivare ad un nodo dell'albero
#relerror ? l'errore relativo
#xerror e xstd sono media e deviazione standard del cross-validation error
#(rpart effettua al suo interno una cross-validation)
printcp(pt1.cart)
#Visualizzo graficamente l'albero
rpart.plot(pt1.cart)
#Le informazioni sul costo CP in funzione del numero di splitting sono contenute nella CP table
breast.cart$cptable
pt1.cart$cptable
plotcp(breast.cart)
plotcp(pt1.cart)
#Visualizzo graficamente l'albero
rpart.plot(pt1.cart)
#Visualizzo graficamente l'albero
rpart.plot(pt1.cart)
#Selezioniamo il CP col numero minimo k di split che garantisce un errore relativo accettabile,
#Scegliamo il minimo k tale che relerror+xstd < xerror
#Nel nostro caso k=1 (ovvero size of tree=2).
best.cp <- pt1.cart$cptable[2,"CP"]
#Effettuiamo il pruning dell'albero con CP associato
pt1.cart.pruned <- prune(pt1.cart, cp=best.cp)
#Visualizziamo l'albero ottenuto
rpart.plot(pt1.cart.pruned, type = 0, extra = 104)
#Selezioniamo il CP col numero minimo k di split che garantisce un errore relativo accettabile,
#Scegliamo il minimo k tale che relerror+xstd < xerror
#Nel nostro caso k=1 (ovvero size of tree=2).
best.cp <- pt1.cart$cptable[20,"CP"]
#Effettuiamo il pruning dell'albero con CP associato
pt1.cart.pruned <- prune(pt1.cart, cp=best.cp)
#Visualizziamo l'albero ottenuto
rpart.plot(pt1.cart.pruned, type = 0, extra = 104)
#Visualizzo graficamente l'albero
rpart.plot(pt1.cart)
#Selezioniamo il CP col numero minimo k di split che garantisce un errore relativo accettabile,
#Scegliamo il minimo k tale che relerror+xstd < xerror
#Nel nostro caso k=1 (ovvero size of tree=2).
best.cp <- pt1.cart$cptable[20,"CP"]
#Effettuiamo il pruning dell'albero con CP associato
pt1.cart.pruned <- prune(pt1.cart, cp=best.cp)
#Visualizziamo l'albero ottenuto
rpart.plot(pt1.cart.pruned, type = 0, extra = 104)
#Selezioniamo il CP col numero minimo k di split che garantisce un errore relativo accettabile,
#Scegliamo il minimo k tale che relerror+xstd < xerror
#Nel nostro caso k=1 (ovvero size of tree=2).
best.cp <- pt1.cart$cptable[200,"CP"]
#Effettuiamo il pruning dell'albero con CP associato
pt1.cart.pruned <- prune(pt1.cart, cp=best.cp)
#Visualizziamo l'albero ottenuto
rpart.plot(pt1.cart.pruned, type = 0, extra = 104)
pt1.training
View(pt1.training)
printcp
#Per avere maggiori informazioni pi? accurate si pu? usare il metodo "printcp"
#che mostra anche il cost-complexity pruning (CP)
#nsplit ? il numero di split necessari per arrivare ad un nodo dell'albero
#relerror ? l'errore relativo
#xerror e xstd sono media e deviazione standard del cross-validation error
#(rpart effettua al suo interno una cross-validation)
printcp(pt1.cart)
#Carico le libreria necessaria
library(rpart)
#Carichiamo anche rpart.plot per la visualizzazione
library(rpart.plot)
#Vogliamo classificare la tipologia di tumore (campo "Class" in funzione degli altri attributi)
pt1.cart <- rpart(Class ~ ., data = pt1.training, cp = 0.008)
#Stampo l'albero decisionale ottenuto in forma testuale
pt1.cart
#Per avere maggiori informazioni pi? accurate si pu? usare il metodo "printcp"
#che mostra anche il cost-complexity pruning (CP)
#nsplit ? il numero di split necessari per arrivare ad un nodo dell'albero
#relerror ? l'errore relativo
#xerror e xstd sono media e deviazione standard del cross-validation error
#(rpart effettua al suo interno una cross-validation)
printcp(pt1.cart)
#Visualizzo graficamente l'albero
rpart.plot(pt1.cart)
#Vogliamo classificare la tipologia di tumore (campo "Class" in funzione degli altri attributi)
pt1.cart <- rpart(Class ~ ., data = pt1.training, cp = 0.00008)
#Stampo l'albero decisionale ottenuto in forma testuale
pt1.cart
#Per avere maggiori informazioni pi? accurate si pu? usare il metodo "printcp"
#che mostra anche il cost-complexity pruning (CP)
#nsplit ? il numero di split necessari per arrivare ad un nodo dell'albero
#relerror ? l'errore relativo
#xerror e xstd sono media e deviazione standard del cross-validation error
#(rpart effettua al suo interno una cross-validation)
printcp(pt1.cart)
#Visualizzo graficamente l'albero
rpart.plot(pt1.cart)
#Selezioniamo il CP col numero minimo k di split che garantisce un errore relativo accettabile,
#Scegliamo il minimo k tale che relerror+xstd < xerror
#Nel nostro caso k=1 (ovvero size of tree=2).
best.cp <- pt1.cart$cptable[200,"CP"]
#Vogliamo classificare la tipologia di tumore (campo "Class" in funzione degli altri attributi)
pt1.cart <- rpart(Class ~ ., data = pt1.training, cp = 0.0008)
#Stampo l'albero decisionale ottenuto in forma testuale
pt1.cart
#Per avere maggiori informazioni pi? accurate si pu? usare il metodo "printcp"
#che mostra anche il cost-complexity pruning (CP)
#nsplit ? il numero di split necessari per arrivare ad un nodo dell'albero
#relerror ? l'errore relativo
#xerror e xstd sono media e deviazione standard del cross-validation error
#(rpart effettua al suo interno una cross-validation)
printcp(pt1.cart)
#Visualizzo graficamente l'albero
rpart.plot(pt1.cart)
#Vogliamo classificare la tipologia di tumore (campo "Class" in funzione degli altri attributi)
pt1.cart <- rpart(Class ~ ., data = pt1.training, cp = 0.008)
#Stampo l'albero decisionale ottenuto in forma testuale
pt1.cart
#Per avere maggiori informazioni pi? accurate si pu? usare il metodo "printcp"
#che mostra anche il cost-complexity pruning (CP)
#nsplit ? il numero di split necessari per arrivare ad un nodo dell'albero
#relerror ? l'errore relativo
#xerror e xstd sono media e deviazione standard del cross-validation error
#(rpart effettua al suo interno una cross-validation)
printcp(pt1.cart)
#Visualizzo graficamente l'albero
rpart.plot(pt1.cart)
#Selezioniamo il CP col numero minimo k di split che garantisce un errore relativo accettabile,
#Scegliamo il minimo k tale che relerror+xstd < xerror
#Nel nostro caso k=1 (ovvero size of tree=2).
best.cp <- pt1.cart$cptable[200,"CP"]
#Vogliamo classificare la tipologia di tumore (campo "Class" in funzione degli altri attributi)
pt1.cart <- rpart(Class ~ ., data = pt1.training, cp = 0.006)
#Stampo l'albero decisionale ottenuto in forma testuale
pt1.cart
#Per avere maggiori informazioni pi? accurate si pu? usare il metodo "printcp"
#che mostra anche il cost-complexity pruning (CP)
#nsplit ? il numero di split necessari per arrivare ad un nodo dell'albero
#relerror ? l'errore relativo
#xerror e xstd sono media e deviazione standard del cross-validation error
#(rpart effettua al suo interno una cross-validation)
printcp(pt1.cart)
#Visualizzo graficamente l'albero
rpart.plot(pt1.cart)
#Selezioniamo il CP col numero minimo k di split che garantisce un errore relativo accettabile,
#Scegliamo il minimo k tale che relerror+xstd < xerror
#Nel nostro caso k=1 (ovvero size of tree=2).
best.cp <- pt1.cart$cptable[200,"CP"]
#Effettuiamo il pruning dell'albero con CP associato
pt1.cart.pruned <- prune(pt1.cart, cp=best.cp)
#Vogliamo classificare la tipologia di tumore (campo "Class" in funzione degli altri attributi)
pt1.cart <- rpart(Class ~ ., data = pt1.training, cp = 0.005)
#Stampo l'albero decisionale ottenuto in forma testuale
pt1.cart
#Per avere maggiori informazioni pi? accurate si pu? usare il metodo "printcp"
#che mostra anche il cost-complexity pruning (CP)
#nsplit ? il numero di split necessari per arrivare ad un nodo dell'albero
#relerror ? l'errore relativo
#xerror e xstd sono media e deviazione standard del cross-validation error
#(rpart effettua al suo interno una cross-validation)
printcp(pt1.cart)
#Visualizzo graficamente l'albero
rpart.plot(pt1.cart)
#In alternativa possiamo usare la funzione plotcp per visualizzare l'errore relativo in
#funzione del costo CP
plotcp(breast.cart)
#Visualizzo graficamente l'albero
rpart.plot(pt1.cart)
#Selezioniamo il CP col numero minimo k di split che garantisce un errore relativo accettabile,
#Scegliamo il minimo k tale che relerror+xstd < xerror
#Nel nostro caso k=1 (ovvero size of tree=2).
best.cp <- pt1.cart$cptable[200,"CP"]
#Visualizzo graficamente l'albero
rpart.plot(pt1.cart)
#Selezioniamo il CP col numero minimo k di split che garantisce un errore relativo accettabile,
#Scegliamo il minimo k tale che relerror+xstd < xerror
#Nel nostro caso k=1 (ovvero size of tree=2).
best.cp <- pt1.cart$cptable[2,"CP"]
#Effettuiamo il pruning dell'albero con CP associato
pt1.cart.pruned <- prune(pt1.cart, cp=best.cp)
#Visualizziamo l'albero ottenuto
rpart.plot(pt1.cart.pruned, type = 0, extra = 104)
#Visualizzo graficamente l'albero
rpart.plot(pt1.cart)
#Selezioniamo il CP col numero minimo k di split che garantisce un errore relativo accettabile,
#Scegliamo il minimo k tale che relerror+xstd < xerror
#Nel nostro caso k=1 (ovvero size of tree=2).
best.cp <- pt1.cart$cptable[3,"CP"]
#Effettuiamo il pruning dell'albero con CP associato
pt1.cart.pruned <- prune(pt1.cart, cp=best.cp)
#Visualizziamo l'albero ottenuto
rpart.plot(pt1.cart.pruned, type = 0, extra = 104)
#Selezioniamo il CP col numero minimo k di split che garantisce un errore relativo accettabile,
#Scegliamo il minimo k tale che relerror+xstd < xerror
#Nel nostro caso k=1 (ovvero size of tree=2).
best.cp <- pt1.cart$cptable[4,"CP"]
#Effettuiamo il pruning dell'albero con CP associato
pt1.cart.pruned <- prune(pt1.cart, cp=best.cp)
#Visualizziamo l'albero ottenuto
rpart.plot(pt1.cart.pruned, type = 0, extra = 104)
#Selezioniamo il CP col numero minimo k di split che garantisce un errore relativo accettabile,
#Scegliamo il minimo k tale che relerror+xstd < xerror
#Nel nostro caso k=1 (ovvero size of tree=2).
best.cp <- pt1.cart$cptable[5,"CP"]
#Effettuiamo il pruning dell'albero con CP associato
pt1.cart.pruned <- prune(pt1.cart, cp=best.cp)
#Visualizziamo l'albero ottenuto
rpart.plot(pt1.cart.pruned, type = 0, extra = 104)
#Selezioniamo il CP col numero minimo k di split che garantisce un errore relativo accettabile,
#Scegliamo il minimo k tale che relerror+xstd < xerror
#Nel nostro caso k=1 (ovvero size of tree=2).
best.cp <- pt1.cart$cptable[7,"CP"]
#Effettuiamo il pruning dell'albero con CP associato
pt1.cart.pruned <- prune(pt1.cart, cp=best.cp)
#Visualizziamo l'albero ottenuto
rpart.plot(pt1.cart.pruned, type = 0, extra = 104)
#Visualizzo graficamente l'albero
rpart.plot(pt1.cart)
#Selezioniamo il CP col numero minimo k di split che garantisce un errore relativo accettabile,
#Scegliamo il minimo k tale che relerror+xstd < xerror
#Nel nostro caso k=1 (ovvero size of tree=2).
best.cp <- pt1.cart$cptable[7,"CP"]
#Effettuiamo il pruning dell'albero con CP associato
pt1.cart.pruned <- prune(pt1.cart, cp=best.cp)
#Visualizziamo l'albero ottenuto
rpart.plot(pt1.cart.pruned, type = 0, extra = 104)
#Selezioniamo il CP col numero minimo k di split che garantisce un errore relativo accettabile,
#Scegliamo il minimo k tale che relerror+xstd < xerror
#Nel nostro caso k=1 (ovvero size of tree=2).
best.cp <- pt1.cart$cptable[8,"CP"]
#Effettuiamo il pruning dell'albero con CP associato
pt1.cart.pruned <- prune(pt1.cart, cp=best.cp)
#Visualizziamo l'albero ottenuto
rpart.plot(pt1.cart.pruned, type = 0, extra = 104)
#Visualizzo graficamente l'albero
rpart.plot(pt1.cart)
#Selezioniamo il CP col numero minimo k di split che garantisce un errore relativo accettabile,
#Scegliamo il minimo k tale che relerror+xstd < xerror
#Nel nostro caso k=1 (ovvero size of tree=2).
best.cp <- pt1.cart$cptable[8,"CP"]
#Effettuiamo il pruning dell'albero con CP associato
pt1.cart.pruned <- prune(pt1.cart, cp=best.cp)
#Visualizziamo l'albero ottenuto
rpart.plot(pt1.cart.pruned, type = 0, extra = 104)
#Visualizzo graficamente l'albero
rpart.plot(pt1.cart)
#Selezioniamo il CP col numero minimo k di split che garantisce un errore relativo accettabile,
#Scegliamo il minimo k tale che relerror+xstd < xerror
#Nel nostro caso k=1 (ovvero size of tree=2).
best.cp <- pt1.cart$cptable[10,"CP"]
#Effettuiamo il pruning dell'albero con CP associato
pt1.cart.pruned <- prune(pt1.cart, cp=best.cp)
#Visualizziamo l'albero ottenuto
rpart.plot(pt1.cart.pruned, type = 0, extra = 104)
#Visualizzo graficamente l'albero
rpart.plot(pt1.cart)
#Selezioniamo il CP col numero minimo k di split che garantisce un errore relativo accettabile,
#Scegliamo il minimo k tale che relerror+xstd < xerror
#Nel nostro caso k=1 (ovvero size of tree=2).
best.cp <- pt1.cart$cptable[20,"CP"]
#Effettuiamo il pruning dell'albero con CP associato
pt1.cart.pruned <- prune(pt1.cart, cp=best.cp)
#Visualizziamo l'albero ottenuto
rpart.plot(pt1.cart.pruned, type = 0, extra = 104)
#Selezioniamo il CP col numero minimo k di split che garantisce un errore relativo accettabile,
#Scegliamo il minimo k tale che relerror+xstd < xerror
#Nel nostro caso k=1 (ovvero size of tree=2).
best.cp <- pt1.cart$cptable[1,"CP"]
#Effettuiamo il pruning dell'albero con CP associato
pt1.cart.pruned <- prune(pt1.cart, cp=best.cp)
#Visualizziamo l'albero ottenuto
rpart.plot(pt1.cart.pruned, type = 0, extra = 104)
#Visualizzo graficamente l'albero
rpart.plot(pt1.cart)
#Selezioniamo il CP col numero minimo k di split che garantisce un errore relativo accettabile,
#Scegliamo il minimo k tale che relerror+xstd < xerror
#Nel nostro caso k=1 (ovvero size of tree=2).
best.cp <- pt1.cart$cptable[1,"CP"]
#Effettuiamo il pruning dell'albero con CP associato
pt1.cart.pruned <- prune(pt1.cart, cp=best.cp)
#Visualizziamo l'albero ottenuto
rpart.plot(pt1.cart.pruned, type = 0, extra = 104)
#Visualizzo graficamente l'albero
rpart.plot(pt1.cart)
#Selezioniamo il CP col numero minimo k di split che garantisce un errore relativo accettabile,
#Scegliamo il minimo k tale che relerror+xstd < xerror
#Nel nostro caso k=1 (ovvero size of tree=2).
best.cp <- pt1.cart$cptable[2,"CP"]
#Effettuiamo il pruning dell'albero con CP associato
pt1.cart.pruned <- prune(pt1.cart, cp=best.cp)
#Visualizziamo l'albero ottenuto
rpart.plot(pt1.cart.pruned, type = 0, extra = 104)
#Per avere maggiori informazioni pi? accurate si pu? usare il metodo "printcp"
#che mostra anche il cost-complexity pruning (CP)
#nsplit ? il numero di split necessari per arrivare ad un nodo dell'albero
#relerror ? l'errore relativo
#xerror e xstd sono media e deviazione standard del cross-validation error
#(rpart effettua al suo interno una cross-validation)
printcp(pt1.cart)
#Visualizzo graficamente l'albero
rpart.plot(pt1.cart)
#Selezioniamo il CP col numero minimo k di split che garantisce un errore relativo accettabile,
#Scegliamo il minimo k tale che relerror+xstd < xerror
#Nel nostro caso k=1 (ovvero size of tree=2).
best.cp <- pt1.cart$cptable[10,"CP"]
#Effettuiamo il pruning dell'albero con CP associato
pt1.cart.pruned <- prune(pt1.cart, cp=best.cp)
#Visualizziamo l'albero ottenuto
rpart.plot(pt1.cart.pruned, type = 0, extra = 104)
#Selezioniamo il CP col numero minimo k di split che garantisce un errore relativo accettabile,
#Scegliamo il minimo k tale che relerror+xstd < xerror
#Nel nostro caso k=1 (ovvero size of tree=2).
best.cp <- pt1.cart$cptable[3,"CP"]
#Effettuiamo il pruning dell'albero con CP associato
pt1.cart.pruned <- prune(pt1.cart, cp=best.cp)
#Visualizziamo l'albero ottenuto
rpart.plot(pt1.cart.pruned, type = 0, extra = 104)
#Selezioniamo il CP col numero minimo k di split che garantisce un errore relativo accettabile,
#Scegliamo il minimo k tale che relerror+xstd < xerror
#Nel nostro caso k=1 (ovvero size of tree=2).
best.cp <- pt1.cart$cptable[4,"CP"]
#Effettuiamo il pruning dell'albero con CP associato
pt1.cart.pruned <- prune(pt1.cart, cp=best.cp)
#Visualizziamo l'albero ottenuto
rpart.plot(pt1.cart.pruned, type = 0, extra = 104)
#Selezioniamo il CP col numero minimo k di split che garantisce un errore relativo accettabile,
#Scegliamo il minimo k tale che relerror+xstd < xerror
#Nel nostro caso k=1 (ovvero size of tree=2).
best.cp <- pt1.cart$cptable[5,"CP"]
#Effettuiamo il pruning dell'albero con CP associato
pt1.cart.pruned <- prune(pt1.cart, cp=best.cp)
#Visualizziamo l'albero ottenuto
rpart.plot(pt1.cart.pruned, type = 0, extra = 104)
#Selezioniamo il CP col numero minimo k di split che garantisce un errore relativo accettabile,
#Scegliamo il minimo k tale che relerror+xstd < xerror
#Nel nostro caso k=1 (ovvero size of tree=2).
best.cp <- pt1.cart$cptable[6,"CP"]
#Effettuiamo il pruning dell'albero con CP associato
pt1.cart.pruned <- prune(pt1.cart, cp=best.cp)
#Visualizziamo l'albero ottenuto
rpart.plot(pt1.cart.pruned, type = 0, extra = 104)
#Selezioniamo il CP col numero minimo k di split che garantisce un errore relativo accettabile,
#Scegliamo il minimo k tale che relerror+xstd < xerror
#Nel nostro caso k=1 (ovvero size of tree=2).
best.cp <- pt1.cart$cptable[5,"CP"]
#Effettuiamo il pruning dell'albero con CP associato
pt1.cart.pruned <- prune(pt1.cart, cp=best.cp)
#Visualizziamo l'albero ottenuto
rpart.plot(pt1.cart.pruned, type = 0, extra = 104)
#Selezioniamo il CP col numero minimo k di split che garantisce un errore relativo accettabile,
#Scegliamo il minimo k tale che relerror+xstd < xerror
#Nel nostro caso k=1 (ovvero size of tree=2).
best.cp <- pt1.cart$cptable[4,"CP"]
#Effettuiamo il pruning dell'albero con CP associato
pt1.cart.pruned <- prune(pt1.cart, cp=best.cp)
#Visualizziamo l'albero ottenuto
rpart.plot(pt1.cart.pruned, type = 0, extra = 104)
#Selezioniamo il CP col numero minimo k di split che garantisce un errore relativo accettabile,
#Scegliamo il minimo k tale che relerror+xstd < xerror
#Nel nostro caso k=1 (ovvero size of tree=2).
best.cp <- pt1.cart$cptable[6,"CP"]
#Effettuiamo il pruning dell'albero con CP associato
pt1.cart.pruned <- prune(pt1.cart, cp=best.cp)
#Visualizziamo l'albero ottenuto
rpart.plot(pt1.cart.pruned, type = 0, extra = 104)
#Vogliamo classificare la tipologia di tumore (campo "Class" in funzione degli altri attributi)
pt1.cart <- rpart(Class ~ ., data = pt1.training, cp = 0.008)
#Stampo l'albero decisionale ottenuto in forma testuale
pt1.cart
#Per avere maggiori informazioni pi? accurate si pu? usare il metodo "printcp"
#che mostra anche il cost-complexity pruning (CP)
#nsplit ? il numero di split necessari per arrivare ad un nodo dell'albero
#relerror ? l'errore relativo
#xerror e xstd sono media e deviazione standard del cross-validation error
#(rpart effettua al suo interno una cross-validation)
printcp(pt1.cart)
#Visualizzo graficamente l'albero
rpart.plot(pt1.cart)
#Selezioniamo il CP col numero minimo k di split che garantisce un errore relativo accettabile,
#Scegliamo il minimo k tale che relerror+xstd < xerror
#Nel nostro caso k=1 (ovvero size of tree=2).
best.cp <- pt1.cart$cptable[6,"CP"]
#Vogliamo classificare la tipologia di tumore (campo "Class" in funzione degli altri attributi)
pt1.cart <- rpart(Class ~ ., data = pt1.training, cp = 0.007)
#Stampo l'albero decisionale ottenuto in forma testuale
pt1.cart
#Per avere maggiori informazioni pi? accurate si pu? usare il metodo "printcp"
#che mostra anche il cost-complexity pruning (CP)
#nsplit ? il numero di split necessari per arrivare ad un nodo dell'albero
#relerror ? l'errore relativo
#xerror e xstd sono media e deviazione standard del cross-validation error
#(rpart effettua al suo interno una cross-validation)
printcp(pt1.cart)
#Visualizzo graficamente l'albero
rpart.plot(pt1.cart)
#Selezioniamo il CP col numero minimo k di split che garantisce un errore relativo accettabile,
#Scegliamo il minimo k tale che relerror+xstd < xerror
#Nel nostro caso k=1 (ovvero size of tree=2).
best.cp <- pt1.cart$cptable[6,"CP"]
#Selezioniamo il CP col numero minimo k di split che garantisce un errore relativo accettabile,
#Scegliamo il minimo k tale che relerror+xstd < xerror
#Nel nostro caso k=1 (ovvero size of tree=2).
best.cp <- pt1.cart$cptable[7,"CP"]
#Effettuiamo il pruning dell'albero con CP associato
pt1.cart.pruned <- prune(pt1.cart, cp=best.cp)
#Visualizziamo l'albero ottenuto
rpart.plot(pt1.cart.pruned, type = 0, extra = 104)
#Selezioniamo il CP col numero minimo k di split che garantisce un errore relativo accettabile,
#Scegliamo il minimo k tale che relerror+xstd < xerror
#Nel nostro caso k=1 (ovvero size of tree=2).
best.cp <- pt1.cart$cptable[8,"CP"]
#Effettuiamo il pruning dell'albero con CP associato
pt1.cart.pruned <- prune(pt1.cart, cp=best.cp)
#Visualizziamo l'albero ottenuto
rpart.plot(pt1.cart.pruned, type = 0, extra = 104)
#Vogliamo classificare la tipologia di tumore (campo "Class" in funzione degli altri attributi)
pt1.cart <- rpart(Class ~ ., data = pt1.training, cp = 0.006)
#Stampo l'albero decisionale ottenuto in forma testuale
pt1.cart
#Per avere maggiori informazioni pi? accurate si pu? usare il metodo "printcp"
#che mostra anche il cost-complexity pruning (CP)
#nsplit ? il numero di split necessari per arrivare ad un nodo dell'albero
#relerror ? l'errore relativo
#xerror e xstd sono media e deviazione standard del cross-validation error
#(rpart effettua al suo interno una cross-validation)
printcp(pt1.cart)
#Visualizzo graficamente l'albero
rpart.plot(pt1.cart)
#Selezioniamo il CP col numero minimo k di split che garantisce un errore relativo accettabile,
#Scegliamo il minimo k tale che relerror+xstd < xerror
#Nel nostro caso k=1 (ovvero size of tree=2).
best.cp <- pt1.cart$cptable[6,"CP"]
#Vogliamo classificare la tipologia di tumore (campo "Class" in funzione degli altri attributi)
pt1.cart <- rpart(Class ~ ., data = pt1.training, cp = 0.005)
#Stampo l'albero decisionale ottenuto in forma testuale
pt1.cart
#Per avere maggiori informazioni pi? accurate si pu? usare il metodo "printcp"
#che mostra anche il cost-complexity pruning (CP)
#nsplit ? il numero di split necessari per arrivare ad un nodo dell'albero
#relerror ? l'errore relativo
#xerror e xstd sono media e deviazione standard del cross-validation error
#(rpart effettua al suo interno una cross-validation)
printcp(pt1.cart)
#Visualizzo graficamente l'albero
rpart.plot(pt1.cart)
#Selezioniamo il CP col numero minimo k di split che garantisce un errore relativo accettabile,
#Scegliamo il minimo k tale che relerror+xstd < xerror
#Nel nostro caso k=1 (ovvero size of tree=2).
best.cp <- pt1.cart$cptable[6,"CP"]
#Effettuiamo il pruning dell'albero con CP associato
pt1.cart.pruned <- prune(pt1.cart, cp=best.cp)
#Visualizziamo l'albero ottenuto
rpart.plot(pt1.cart.pruned, type = 0, extra = 104)
#Vogliamo classificare la tipologia di tumore (campo "Class" in funzione degli altri attributi)
pt1.cart <- rpart(Class ~ ., data = pt1.training, cp = 0.004)
#Stampo l'albero decisionale ottenuto in forma testuale
pt1.cart
#Per avere maggiori informazioni pi? accurate si pu? usare il metodo "printcp"
#che mostra anche il cost-complexity pruning (CP)
#nsplit ? il numero di split necessari per arrivare ad un nodo dell'albero
#relerror ? l'errore relativo
#xerror e xstd sono media e deviazione standard del cross-validation error
#(rpart effettua al suo interno una cross-validation)
printcp(pt1.cart)
#Visualizzo graficamente l'albero
rpart.plot(pt1.cart)
#Selezioniamo il CP col numero minimo k di split che garantisce un errore relativo accettabile,
#Scegliamo il minimo k tale che relerror+xstd < xerror
#Nel nostro caso k=1 (ovvero size of tree=2).
best.cp <- pt1.cart$cptable[6,"CP"]
#Effettuiamo il pruning dell'albero con CP associato
pt1.cart.pruned <- prune(pt1.cart, cp=best.cp)
#Visualizziamo l'albero ottenuto
rpart.plot(pt1.cart.pruned, type = 0, extra = 104)
#Vogliamo classificare la tipologia di tumore (campo "Class" in funzione degli altri attributi)
pt1.cart <- rpart(Class ~ ., data = pt1.training, cp = 0.005)
#Stampo l'albero decisionale ottenuto in forma testuale
pt1.cart
#Per avere maggiori informazioni pi? accurate si pu? usare il metodo "printcp"
#che mostra anche il cost-complexity pruning (CP)
#nsplit ? il numero di split necessari per arrivare ad un nodo dell'albero
#relerror ? l'errore relativo
#xerror e xstd sono media e deviazione standard del cross-validation error
#(rpart effettua al suo interno una cross-validation)
printcp(pt1.cart)
#Visualizzo graficamente l'albero
rpart.plot(pt1.cart)
#Selezioniamo il CP col numero minimo k di split che garantisce un errore relativo accettabile,
#Scegliamo il minimo k tale che relerror+xstd < xerror
#Nel nostro caso k=1 (ovvero size of tree=2).
best.cp <- pt1.cart$cptable[6,"CP"]
#Effettuiamo il pruning dell'albero con CP associato
pt1.cart.pruned <- prune(pt1.cart, cp=best.cp)
#Visualizziamo l'albero ottenuto
rpart.plot(pt1.cart.pruned, type = 0, extra = 104)
